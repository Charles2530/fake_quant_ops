# Fake Quantization Operations

ä¸€ä¸ªé«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ é‡åŒ–æ“ä½œåº“ï¼Œæ”¯æŒå¤šç§é‡åŒ–æ ¼å¼ï¼ˆMXFPã€HiFPç­‰ï¼‰ï¼Œæä¾›CPUã€GPUå’ŒNPUåŠ é€Ÿå®ç°ã€‚

## ğŸ“‹ ç›®å½•

- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [æ”¯æŒçš„é‡åŒ–æ ¼å¼](#æ”¯æŒçš„é‡åŒ–æ ¼å¼)
- [å®‰è£…](#å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [APIæ–‡æ¡£](#apiæ–‡æ¡£)
- [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)
- [è®¸å¯è¯](#è®¸å¯è¯)

## âœ¨ åŠŸèƒ½ç‰¹æ€§

- **å¤šç§é‡åŒ–æ ¼å¼æ”¯æŒ**ï¼šMXFP (E4M3/E5M2)ã€HiFPã€FP8ã€FP4ç­‰
- **å¤šå¹³å°æ”¯æŒ**ï¼šCPUã€GPU (CUDA)ã€NPU (Ascend 910B)
- **é«˜æ•ˆå®ç°**ï¼šä¼˜åŒ–çš„é‡åŒ–ç®—æ³•ï¼Œæ”¯æŒå—çº§å…±äº«æŒ‡æ•°
- **PyTorché›†æˆ**ï¼šæ— ç¼é›†æˆPyTorchï¼Œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼
- **çµæ´»é…ç½®**ï¼šå¯è‡ªå®šä¹‰é‡åŒ–å‚æ•°ï¼ˆæŒ‡æ•°ä½ã€å°¾æ•°ä½ã€å—å¤§å°ç­‰ï¼‰

## ğŸ¯ æ”¯æŒçš„é‡åŒ–æ ¼å¼

### MXFP (Mixed-Precision Floating Point)
- **MXFP8 E4M3**: 8ä½ï¼Œ4ä½æŒ‡æ•°ï¼Œ3ä½å°¾æ•°
- **MXFP8 E5M2**: 8ä½ï¼Œ5ä½æŒ‡æ•°ï¼Œ2ä½å°¾æ•°
- **MXFP6 E3M2**: 6ä½ï¼Œ3ä½æŒ‡æ•°ï¼Œ2ä½å°¾æ•°
- **MXFP4 E2M1**: 4ä½ï¼Œ2ä½æŒ‡æ•°ï¼Œ1ä½å°¾æ•°

### HiFP (Hierarchical Floating Point)
- **HiF8**: 8ä½æ··åˆç²¾åº¦æµ®ç‚¹é‡åŒ–
- **HiF4**: 4ä½æ··åˆç²¾åº¦æµ®ç‚¹é‡åŒ– (hifx4_v12)
- **HiF2/3/5**: æ”¯æŒ2-5ä½å˜ä½“

### å…¶ä»–æ ¼å¼
- **FP16/BF16**: æ ‡å‡†åŠç²¾åº¦æµ®ç‚¹
- **NVF4**: 4ä½NVæµ®ç‚¹é‡åŒ–
- **NF4**: 4ä½NormalFloaté‡åŒ–

## ğŸš€ å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python 3.7+
- PyTorch 1.8+
- NumPy

### åŸºç¡€å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourusername/fake_quant_ops.git
cd fake_quant_ops

# å®‰è£…ä¾èµ–
pip install torch numpy
```

### NPUæ”¯æŒï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦NPUåŠ é€Ÿæ”¯æŒï¼ˆAscend 910Bï¼‰ï¼Œéœ€è¦é¢å¤–å®‰è£…ï¼š

```bash
# å®‰è£…torch_npu
pip install torch_npu

# ç¼–è¯‘NPUç®—å­
cd utils/quant_cy_npu
./build.sh
```

## ğŸ“– å¿«é€Ÿå¼€å§‹

### åŸºæœ¬é‡åŒ–ç¤ºä¾‹

```python
import torch
from quant.mxfp import _quantize_mx, mxfp_matmul
from quant.hifp import quant_hif8, hifp_matmul
from quant.qtype import QType

# åˆ›å»ºæµ‹è¯•å¼ é‡
x = torch.randn(1024, 1024).cuda()

# MXFP8é‡åŒ–
x_mxfp8 = _quantize_mx(
    x, 
    scale_bits=8, 
    elem_format='fp8_e4m3',
    shared_exp_method="max",
    axes=-1,
    block_size=16
)

# HiF8é‡åŒ–
x_hif8 = quant_hif8(x)

print(f"Original shape: {x.shape}")
print(f"MXFP8 shape: {x_mxfp8.shape}")
print(f"HiF8 shape: {x_hif8.shape}")
```

### çŸ©é˜µä¹˜æ³•é‡åŒ–

```python
import torch
from quant.mxfp import mxfp_matmul
from quant.hifp import hifp_matmul

# åˆ›å»ºè¾“å…¥çŸ©é˜µ
A = torch.randn(1024, 1024).cuda()
B = torch.randn(1024, 1024).cuda()

# MXFP8çŸ©é˜µä¹˜æ³•
C_mxfp8 = mxfp_matmul(A, B, elem_format='fp8_e4m3', block_size=32)

# HiF8çŸ©é˜µä¹˜æ³•
C_hif8 = hifp_matmul(A, B)

# å¯¹æ¯”ç²¾åº¦
C_fp32 = torch.matmul(A, B)
print(f"FP32 vs MXFP8 MSE: {torch.mean((C_fp32 - C_mxfp8) ** 2).item():.6f}")
print(f"FP32 vs HiF8 MSE: {torch.mean((C_fp32 - C_hif8) ** 2).item():.6f}")
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### 1. ä½¿ç”¨QTypeå®šä¹‰é‡åŒ–ç±»å‹

```python
from quant.qtype import QType

# å®šä¹‰MXFP4é‡åŒ–ç±»å‹
qtype_mxfp4 = QType('mxfp4')  # ç­‰ä»·äº e2m1k8b32c

# å®šä¹‰MXFP8 E4M3é‡åŒ–ç±»å‹
qtype_mxfp8_e4m3 = QType('mxfp8e4m3')  # ç­‰ä»·äº e4m3k8b32c

# å®šä¹‰HiF8é‡åŒ–ç±»å‹
qtype_hif8 = QType('hif8')

# å®šä¹‰è‡ªå®šä¹‰é‡åŒ–ç±»å‹
qtype_custom = QType('e3m2k8b16c')  # 3ä½æŒ‡æ•°ï¼Œ2ä½å°¾æ•°ï¼Œ8ä½å…±äº«æŒ‡æ•°ï¼Œå—å¤§å°16

# æŒ‡å®šé‡åŒ–ç»´åº¦
qtype_with_dim = QType('hif8').dim(-1)  # åœ¨æœ€åä¸€ä¸ªç»´åº¦é‡åŒ–
```

### 2. QKVé‡åŒ–ï¼ˆç”¨äºTransformerï¼‰

```python
from quant.mxfp import quant_dequant_qkv

# å‡è®¾q, k, væ˜¯Transformerçš„query, key, valueå¼ é‡
q = torch.randn(32, 128, 1024).cuda()
k = torch.randn(32, 128, 1024).cuda()
v = torch.randn(32, 128, 1024).cuda()

# é‡åŒ–QKVï¼ˆä¿æŒæ¢¯åº¦ï¼‰
q_q, k_q, v_q = quant_dequant_qkv(q, k, v, elem_format='fp8_e4m3')
```

### 3. é€šç”¨å¼ é‡é‡åŒ–

```python
from quant.mxfp import quant_dequant_tensor

x = torch.randn(1024, 1024).cuda()
x_quantized = quant_dequant_tensor(x, elem_format='fp8_e5m2')
```

### 4. æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆBAddBmmï¼‰

```python
from quant.mxfp import mxfp_baddbmm
from quant.hifp import hifp_baddbmm

# æ‰¹é‡çŸ©é˜µä¹˜æ³•
batch1 = torch.randn(10, 1024, 512).cuda()
batch2 = torch.randn(10, 512, 1024).cuda()
input_tensor = torch.randn(10, 1024, 1024).cuda()

# MXFP8æ‰¹é‡çŸ©é˜µä¹˜æ³•
output_mxfp = mxfp_baddbmm(
    input_tensor, batch1, batch2,
    beta=1.0, alpha=1.0,
    elem_format='fp8_e4m3',
    block_size=32
)

# HiF8æ‰¹é‡çŸ©é˜µä¹˜æ³•
output_hif = hifp_baddbmm(input_tensor, batch1, batch2, beta=1.0, alpha=1.0)
```

### 5. é‡åŒ–è¯¯å·®åˆ†æ

```python
import torch
from quant.mxfp import _quantize_mx
from quant.hifp import quant_hif8

x = torch.randn(1024, 1024).cuda()

# MXFP8é‡åŒ–
x_mxfp8 = _quantize_mx(
    x, scale_bits=8, elem_format='fp8_e4m3',
    shared_exp_method="max", axes=-1, block_size=16
)

# HiF8é‡åŒ–
x_hif8 = quant_hif8(x)

# è®¡ç®—è¯¯å·®
mse_mxfp8 = torch.mean((x - x_mxfp8) ** 2)
mse_hif8 = torch.mean((x - x_hif8) ** 2)
max_err_mxfp8 = torch.max(torch.abs(x - x_mxfp8))
max_err_hif8 = torch.max(torch.abs(x - x_hif8))

print(f"MXFP8 - MSE: {mse_mxfp8.item():.6f}, Max Error: {max_err_mxfp8.item():.6f}")
print(f"HiF8  - MSE: {mse_hif8.item():.6f}, Max Error: {max_err_hif8.item():.6f}")
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
fake_quant_ops/
â”œâ”€â”€ quant/                    # åŸºç¡€é‡åŒ–å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ qtype.py             # é‡åŒ–ç±»å‹å®šä¹‰
â”‚   â”œâ”€â”€ mxfp.py              # MXFPé‡åŒ–å®ç°
â”‚   â””â”€â”€ hifp.py              # HiFPé‡åŒ–å®ç°
â”‚
â”œâ”€â”€ quant_npu/               # NPUç›¸å…³é‡åŒ–å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ qtype.py             # NPUé‡åŒ–ç±»å‹
â”‚   â”œâ”€â”€ mxfp_npu.py          # NPU MXFPå®ç°
â”‚   â””â”€â”€ hifp_npu.py          # NPU HiFPå®ç°
â”‚
â”œâ”€â”€ utils/                   # å·¥å…·å’Œæµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ test_dtype.py        # é‡åŒ–è¯¯å·®æµ‹è¯•
â”‚   â”œâ”€â”€ mxfp_scaling_test.py # MXFPç¼©æ”¾æµ‹è¯•
â”‚   â”œâ”€â”€ plot_loss_curve.py   # æŸå¤±æ›²çº¿ç»˜åˆ¶
â”‚   â”œâ”€â”€ saver/               # é‡åŒ–ä¿å­˜å™¨
â”‚   â”‚   â”œâ”€â”€ mxfp_saver.py
â”‚   â”‚   â”œâ”€â”€ hifp_saver.py
â”‚   â”‚   â””â”€â”€ bf16_saver.py
â”‚   â””â”€â”€ quant_cy_npu/        # NPU C++æ‰©å±•
â”‚       â”œâ”€â”€ setup.py
â”‚       â”œâ”€â”€ build.sh
â”‚       â”œâ”€â”€ README.md
â”‚       â””â”€â”€ quant_cy_npu/
â”‚           â””â”€â”€ base/
â”‚               â”œâ”€â”€ QType.py
â”‚               â”œâ”€â”€ QTensor.py
â”‚               â””â”€â”€ QFunc/
â”‚
â””â”€â”€ README.md                # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## ğŸ“š APIæ–‡æ¡£

### MXFPé‡åŒ–

#### `_quantize_mx(A, scale_bits, elem_format, shared_exp_method, axes, block_size, round, flush_fp32_subnorms, minus_exp)`

MXFPé‡åŒ–æ ¸å¿ƒå‡½æ•°ã€‚

**å‚æ•°ï¼š**
- `A`: è¾“å…¥å¼ é‡
- `scale_bits`: å…±äº«æŒ‡æ•°ä½æ•°ï¼ˆé€šå¸¸ä¸º8ï¼‰
- `elem_format`: å…ƒç´ æ ¼å¼ï¼ˆ'fp8_e4m3', 'fp8_e5m2', 'fp6_e3m2', 'fp4_e2m1'ç­‰ï¼‰
- `shared_exp_method`: å…±äº«æŒ‡æ•°é€‰æ‹©æ–¹æ³•ï¼ˆ'max'æˆ–'none'ï¼‰
- `axes`: å…±äº«æŒ‡æ•°çš„è½´
- `block_size`: å—å¤§å°ï¼ˆ0è¡¨ç¤ºä¸ä½¿ç”¨å—ï¼‰
- `round`: èˆå…¥æ–¹æ³•ï¼ˆ'nearest', 'floor', 'even', 'dither'ï¼‰
- `flush_fp32_subnorms`: æ˜¯å¦å°†FP32æ¬¡æ­£è§„æ•°åˆ·æ–°ä¸º0
- `minus_exp`: æŒ‡æ•°åç§»é‡

**è¿”å›ï¼š** é‡åŒ–åçš„å¼ é‡

#### `mxfp_matmul(A, B, elem_format='fp8_e5m2', block_size=32)`

MXFPçŸ©é˜µä¹˜æ³•ï¼Œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼ã€‚

#### `mxfp_baddbmm(input, batch1, batch2, beta=1.0, alpha=1.0, elem_format='fp8_e5m2', block_size=32)`

MXFPæ‰¹é‡çŸ©é˜µä¹˜æ³•ã€‚

### HiFPé‡åŒ–

#### `quant_hif8(x, Q=None, qdim=-1)`

HiF8é‡åŒ–å‡½æ•°ã€‚

**å‚æ•°ï¼š**
- `x`: è¾“å…¥å¼ é‡
- `Q`: QTypeå¯¹è±¡ï¼ˆå¯é€‰ï¼‰
- `qdim`: é‡åŒ–ç»´åº¦

**è¿”å›ï¼š** é‡åŒ–åçš„å¼ é‡

#### `hifp_matmul(A, B)`

HiF8çŸ©é˜µä¹˜æ³•ã€‚

#### `hifp_baddbmm(input, batch1, batch2, beta=1.0, alpha=1.0)`

HiF8æ‰¹é‡çŸ©é˜µä¹˜æ³•ã€‚

### QTypeç±»

#### `QType(desc)`

é‡åŒ–ç±»å‹å®šä¹‰ç±»ã€‚

**æ”¯æŒçš„æ ¼å¼ï¼š**
- `'mxfp4'`, `'mxfp6e3m2'`, `'mxfp8e4m3'`, `'mxfp8e5m2'`
- `'hif8'`, `'hifx2_v12'`, `'hifx3_v12'`, `'hifx4_v12'`, `'hifx5_v12'`
- `'fp16'`, `'fp32'`, `'bf16'`
- `'nvf4'`
- è‡ªå®šä¹‰æ ¼å¼ï¼š`'e<exp_bits>m<man_bits>k<k_bits>b<block_size>[c]'`

**æ–¹æ³•ï¼š**
- `dim(dim)`: è®¾ç½®é‡åŒ–ç»´åº¦ï¼ˆè¿”å›æ–°å¯¹è±¡ï¼‰
- `dim_(dim)`: è®¾ç½®é‡åŒ–ç»´åº¦ï¼ˆåŸåœ°ä¿®æ”¹ï¼‰
- `copy()`: å¤åˆ¶QTypeå¯¹è±¡

## âš¡ æ€§èƒ½åŸºå‡†

åœ¨NVIDIA A100 GPUä¸Šçš„å…¸å‹æ€§èƒ½è¡¨ç°ï¼š

| é‡åŒ–æ ¼å¼ | è¾“å…¥å¤§å° | é‡åŒ–å»¶è¿Ÿ | å†…å­˜èŠ‚çœ | ç²¾åº¦æŸå¤± (MSE) |
|---------|---------|---------|---------|---------------|
| MXFP8 E4M3 | 1024Ã—1024 | ~0.15ms | 75% | ~1e-4 |
| MXFP8 E5M2 | 1024Ã—1024 | ~0.15ms | 75% | ~1e-3 |
| MXFP4 | 1024Ã—1024 | ~0.12ms | 87.5% | ~1e-2 |
| HiF8 | 1024Ã—1024 | ~0.18ms | 75% | ~5e-4 |
| HiF4 | 1024Ã—1024 | ~0.14ms | 87.5% | ~1e-2 |

*æ³¨ï¼šå®é™…æ€§èƒ½å¯èƒ½å› ç¡¬ä»¶é…ç½®å’Œè½¯ä»¶ç‰ˆæœ¬è€Œå¼‚*

## ğŸ”§ å¼€å‘ä¸æµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•é‡åŒ–è¯¯å·®
python utils/test_dtype.py <tensor_file> --format hifp8

# æµ‹è¯•MXFPç¼©æ”¾
python utils/mxfp_scaling_test.py

# å¿«é€Ÿæµ‹è¯•NPUç®—å­ï¼ˆå¦‚æœå·²å®‰è£…ï¼‰
cd utils/quant_cy_npu
python quick_test.py
```

### æ„å»ºNPUæ‰©å±•

```bash
cd utils/quant_cy_npu
./build.sh
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache License 2.0 è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢æ‰€æœ‰ä¸ºé‡åŒ–æŠ€æœ¯åšå‡ºè´¡çŒ®çš„ç ”ç©¶è€…å’Œå¼€å‘è€…
- ç‰¹åˆ«æ„Ÿè°¢PyTorchå›¢é˜Ÿæä¾›çš„ä¼˜ç§€æ¡†æ¶

## ğŸ“® è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- æäº¤ Issue: [GitHub Issues](https://github.com/yourusername/fake_quant_ops/issues)
- å‘é€é‚®ä»¶: your.email@example.com

---

**æ³¨æ„**: æœ¬é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ï¼ŒAPIå¯èƒ½ä¼šæœ‰å˜åŒ–ã€‚å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å‰è¿›è¡Œå……åˆ†æµ‹è¯•ã€‚



